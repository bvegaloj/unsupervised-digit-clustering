{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.data_loader import load_and_preprocess_mnist\n",
    "from src.models.kmeans import KMeans\n",
    "from src.visualization import plot_clusters\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92479314",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "K-means works with continuous data, so we use grayscale (not binarized) images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89274002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train, y_train, X_test, y_test = load_and_preprocess_mnist(\n",
    "    n_samples=5000,\n",
    "    binarize=False,  # Use grayscale for K-means\n",
    "    flatten=True\n",
    ")\n",
    "\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606cbfe",
   "metadata": {},
   "source": [
    "## 2. Fit K-Means Model\n",
    "\n",
    "We'll cluster into 10 groups (matching the 10 digit classes), though K-means doesn't know about the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit K-means\n",
    "kmeans = KMeans(n_clusters=10, max_iters=300, random_state=42)\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "print(f\"✓ Converged in {kmeans.n_iter_} iterations\")\n",
    "print(f\"✓ Cluster centers shape: {kmeans.cluster_centers_.shape}\")\n",
    "print(f\"\\nCluster sizes:\")\n",
    "unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster}: {count} samples ({count/len(kmeans.labels_)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc52803",
   "metadata": {},
   "source": [
    "## 3. Visualize Learned Cluster Centers\n",
    "\n",
    "Each cluster center represents the \"average\" digit assigned to that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cluster centers\n",
    "fig, axes = plot_clusters(kmeans.cluster_centers_, rows=2)\n",
    "plt.savefig('../results/kmeans_cluster_centers.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"These are the learned cluster prototypes from K-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1623329",
   "metadata": {},
   "source": [
    "## 4. Analyze Cluster-Label Correspondence\n",
    "\n",
    "Since we have ground truth labels, let's see how well clusters align with digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix: rows=true labels, cols=predicted clusters\n",
    "confusion = np.zeros((10, 10), dtype=int)\n",
    "for true_label in range(10):\n",
    "    mask = y_train == true_label\n",
    "    cluster_assignments = kmeans.labels_[mask]\n",
    "    for cluster in range(10):\n",
    "        confusion[true_label, cluster] = np.sum(cluster_assignments == cluster)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(confusion, cmap='Blues', aspect='auto')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        text = ax.text(j, i, confusion[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "ax.set_xlabel('K-Means Cluster', fontsize=12)\n",
    "ax.set_ylabel('True Digit Label', fontsize=12)\n",
    "ax.set_title('Cluster-Label Correspondence Matrix', fontsize=14)\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_yticks(range(10))\n",
    "plt.colorbar(im, ax=ax, label='Number of samples')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/kmeans_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Each row shows how digits with a specific label are distributed across clusters\")\n",
    "print(\"Ideally, each row would have one dominant cluster (perfect separation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3932ec5",
   "metadata": {},
   "source": [
    "## 5. Calculate Clustering Purity\n",
    "\n",
    "**Purity** measures the extent to which clusters contain samples from a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc28ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_purity(labels_true, labels_pred):\n",
    "    \"\"\"Calculate clustering purity\"\"\"\n",
    "    n_clusters = len(np.unique(labels_pred))\n",
    "    n_samples = len(labels_true)\n",
    "    \n",
    "    purity_sum = 0\n",
    "    for cluster in range(n_clusters):\n",
    "        # Get true labels for this cluster\n",
    "        cluster_mask = labels_pred == cluster\n",
    "        cluster_true_labels = labels_true[cluster_mask]\n",
    "        \n",
    "        # Find most common true label in this cluster\n",
    "        if len(cluster_true_labels) > 0:\n",
    "            most_common_count = np.bincount(cluster_true_labels).max()\n",
    "            purity_sum += most_common_count\n",
    "    \n",
    "    return purity_sum / n_samples\n",
    "\n",
    "purity = calculate_purity(y_train, kmeans.labels_)\n",
    "print(f\"Clustering Purity: {purity:.3f}\")\n",
    "print(f\"\\nInterpretation: {purity*100:.1f}% of samples are in clusters dominated by their true class\")\n",
    "print(f\"Random clustering would give ~10% purity (1/10 classes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca984ae6",
   "metadata": {},
   "source": [
    "## 6. Show Sample Cluster Members\n",
    "\n",
    "Let's visualize what's actually in a few clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show samples from 3 clusters\n",
    "clusters_to_show = [0, 5, 9]\n",
    "\n",
    "fig, axes = plt.subplots(len(clusters_to_show), 10, figsize=(15, 5))\n",
    "\n",
    "for row, cluster_id in enumerate(clusters_to_show):\n",
    "    # Get samples from this cluster\n",
    "    cluster_indices = np.where(kmeans.labels_ == cluster_id)[0][:10]\n",
    "    \n",
    "    for col, idx in enumerate(cluster_indices):\n",
    "        axes[row, col].imshow(X_train[idx].reshape(28, 28), cmap='gray')\n",
    "        axes[row, col].axis('off')\n",
    "        if col == 0:\n",
    "            axes[row, col].set_ylabel(f'Cluster {cluster_id}', fontsize=12)\n",
    "\n",
    "plt.suptitle('Sample Members from Selected K-Means Clusters', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/kmeans_cluster_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice some clusters contain similar-looking digits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee2a00",
   "metadata": {},
   "source": [
    "## 7. Test Set Prediction\n",
    "\n",
    "Apply the learned model to unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be864728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "test_predictions = kmeans.predict(X_test)\n",
    "\n",
    "print(f\"Predicted {len(test_predictions)} test samples\")\n",
    "print(f\"Test set cluster distribution:\")\n",
    "unique, counts = np.unique(test_predictions, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster}: {count} samples ({count/len(test_predictions)*100:.1f}%)\")\n",
    "\n",
    "# Calculate test set purity\n",
    "test_purity = calculate_purity(y_test, test_predictions)\n",
    "print(f\"\\nTest Set Purity: {test_purity:.3f}\")\n",
    "print(f\"Training Set Purity: {purity:.3f}\")\n",
    "print(f\"Difference: {abs(test_purity - purity):.3f} (similar performance = good generalization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec43a10",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### K-Means Results:\n",
    "\n",
    "- **Convergence**: Fast convergence in ~30-50 iterations\n",
    "- **Cluster quality**: Purity shows clusters capture digit structure reasonably well\n",
    "- **Hard assignments**: Each sample belongs to exactly one cluster\n",
    "- **Limitations**: \n",
    "  - Cannot express uncertainty (sample might be between clusters)\n",
    "  - Assumes spherical clusters (Euclidean distance)\n",
    "  - Sensitive to initialization (we used random_state for reproducibility)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In the next notebook, we'll compare with **Bernoulli Mixture Model**, which provides:\n",
    "- **Soft assignments** (probability distribution over clusters)\n",
    "- **Generative model** (can sample new data)\n",
    "- **Explicit uncertainty quantification**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
